{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS-pdbnl7X3X",
        "outputId": "395e8185-abd2-48fc-9fe0-c7c3c525058f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Environment-adjusted-Topic-Models'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 16 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (16/16), 25.84 MiB | 8.20 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/anonymousindividual007/Environment-adjusted-Topic-Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlhkhCeBVIIL",
        "outputId": "96382339-b560-4369-a47a-a9d4e32125eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools as it\n",
        "import math\n",
        "import csv\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.distributions as dist\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.distributions import Normal, Distribution, HalfCauchy, Laplace\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "_mKIpYH11tph"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The political_stopwords.txt is used for preprocessing in all of our experiments.\n"
      ],
      "metadata": {
        "id": "7Lptxo8yliDv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A4ebiLxw7i64"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/Environment-adjusted-Topic-Models/political_stopwords.txt\"\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    stopwords_list = file.readlines()\n",
        "\n",
        "all_stopwords = [word.strip() for word in stopwords_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GKibgGW97j9V"
      },
      "outputs": [],
      "source": [
        "class LemmaTokenizer:\n",
        "\tdef __init__(self):\n",
        "\t\tself.wnl = WordNetLemmatizer()\n",
        "\tdef __call__(self, doc):\n",
        "\t\treturn [t for t in word_tokenize(doc) if str.isalpha(t)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use your own data replace the file path. Ensure there is a column called 'source' which indicates the environments of your dataset. In the cell below represents the data is for the Political Advertisements experiment."
      ],
      "metadata": {
        "id": "uroxXKYoltC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/Environment-adjusted-Topic-Models/local_channels.csv'\n",
        "\n",
        "train_data = pd.read_csv(file_path)\n",
        "\n",
        "test1 = train_data[train_data['source'] == 'right'].sample(frac=0.2, random_state=42)\n",
        "test2 = train_data[train_data['source'] == 'left'].sample(frac=0.2, random_state=42)\n",
        "\n",
        "# Drop the sampled rows from train_data\n",
        "train_data = train_data.drop(test1.index)\n",
        "train_data = train_data.drop(test2.index)"
      ],
      "metadata": {
        "id": "bNC5QaY_ldZJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data in the cell below is for the ideology dataset.\n"
      ],
      "metadata": {
        "id": "sAuUYkMXl2Rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data= pd.read_csv('/content/Environment-adjusted-Topic-Models/channels_ideology_train.csv')\n",
        "# channels_ideology_test = pd.read_csv('/content/Environment-adjusted-Topic-Models/channels_ideology_test.csv')\n",
        "# test1 = channels_ideology_test[channels_ideology_test['source'] == 'Republican']\n",
        "# test2 = channels_ideology_test[channels_ideology_test['source'] == 'Democratic']\n",
        "# test3 = channels_ideology_test[channels_ideology_test['source'] == 'balanced']\n"
      ],
      "metadata": {
        "id": "F3sELKmGl07S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), ngram_range=(1, 1), stop_words=all_stopwords, max_df=0.4, min_df=0.0006)\n",
        "\n",
        "docs_word_matrix_raw = vectorizer.fit_transform(train_data['text'])\n",
        "\n",
        "env_mapping = {value: index for index, value in enumerate(train_data['source'].unique())}\n",
        "env_index = train_data['source'].apply(lambda x: env_mapping[x])\n",
        "\n",
        "docs_word_matrix_tensor = torch.from_numpy(docs_word_matrix_raw.toarray()).float().to(device)\n",
        "env_index_tensor = torch.from_numpy(env_index.to_numpy()).long().to(device)\n"
      ],
      "metadata": {
        "id": "H954flOWmLwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below represents the preprocessing for the Style dataset."
      ],
      "metadata": {
        "id": "iGuWi9vomC5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the zip file and the name of the CSV file inside it\n",
        "# zip_file_path = '/content/Environment-adjusted-Topic-Models/style_train_large.csv.zip'\n",
        "# csv_file_name = 'style_train_large.csv'  # Change this if the CSV file has a different name inside the zip\n",
        "\n",
        "# # Specify the temporary directory to extract the CSV file\n",
        "# temp_dir = '/content/temp_dir'\n",
        "\n",
        "# # Create a temporary directory if it doesn't exist\n",
        "# if not os.path.exists(temp_dir):\n",
        "#     os.makedirs(temp_dir)\n",
        "\n",
        "# # Extract the CSV file\n",
        "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "#     zip_ref.extract(csv_file_name, temp_dir)\n",
        "\n",
        "# # Full path to the extracted CSV file\n",
        "# csv_file_path = os.path.join(temp_dir, csv_file_name)\n",
        "\n",
        "# # Load the CSV file into a Pandas DataFrame\n",
        "# train_data = pd.read_csv(csv_file_path, encoding='ISO-8859-1')\n",
        "# style_test_df = pd.read_csv('/content/Environment-adjusted-Topic-Models/style_test.csv', encoding='ISO-8859-1')\n",
        ""
      ],
      "metadata": {
        "id": "XZvm-ekOl8ID"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RC0bsas37pSQ"
      },
      "outputs": [],
      "source": [
        "# # 2. Map the 'source' values to environments\n",
        "# env_map = {'articles': 'env_0', 'speeches': 'env_1', 'tweets': 'env_2'}\n",
        "# style_test_df['source'] = style_test_df['source'].map(env_map)\n",
        "\n",
        "# # Count number of 'articles' and 'speeches' in train_data\n",
        "# num_articles = len(train_data[train_data['source'] == 'articles'])\n",
        "# num_speeches = len(train_data[train_data['source'] == 'speeches'])\n",
        "# num_tweets = len(train_data[train_data['source'] == 'tweets'])\n",
        "\n",
        "\n",
        "# # Determine the lesser count\n",
        "# min_count = min(num_articles, num_speeches, num_tweets)\n",
        "\n",
        "# # Randomly sample that many from both sources\n",
        "# sampled_articles = train_data[train_data['source'] == 'articles'].sample(min_count, random_state=42)\n",
        "# sampled_speeches = train_data[train_data['source'] == 'speeches'].sample(min_count, random_state=42)\n",
        "# sampled_tweets = train_data[train_data['source'] == 'tweets'].sample(min_count, random_state=42)\n",
        "\n",
        "# # Combine the two sampled dataframes to create a balanced train_data\n",
        "# train_data = pd.concat([sampled_articles, sampled_speeches, sampled_tweets], ignore_index=True)\n",
        "\n",
        "# #call it combined for the ood test\n",
        "# # combined_data = pd.concat([sampled_articles, sampled_speeches, sampled_tweets], ignore_index=True)\n",
        "\n",
        "# # # no tweets, but test on tweets\n",
        "# # train_data = pd.concat([sampled_articles, sampled_speeches], ignore_index=True)\n",
        "\n",
        "# # Now, map the 'source' values to the environments (assuming env_map is already defined)\n",
        "\n",
        "# train_data['source'] = train_data['source'].map(env_map)\n",
        "\n",
        "# test1 = style_test_df[style_test_df['source'] == 'env_0']\n",
        "# test2 = style_test_df[style_test_df['source'] == 'env_1']\n",
        "# test3 = style_test_df[style_test_df['source'] == 'env_2']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
        "#                              ngram_range=(1, 1),\n",
        "#                              stop_words=all_stopwords,\n",
        "#                              max_df=0.5,\n",
        "#                              min_df=0.006)\n",
        "\n",
        "# # Fit the vectorizer on the combined dataset\n",
        "# vectorizer.fit(combined_data['text'])\n",
        "\n",
        "# # Only transform the train_data['text'] without fitting again\n",
        "# docs_word_matrix_raw = vectorizer.transform(train_data['text'])\n",
        "\n",
        "# env_mapping = {value: index for index, value in enumerate(train_data['source'].unique())}\n",
        "# env_index = train_data['source'].apply(lambda x: env_mapping[x])\n",
        "\n",
        "# docs_word_matrix_tensor = torch.from_numpy(docs_word_matrix_raw.toarray()).float().to(device)\n",
        "# env_index_tensor = torch.from_numpy(env_index.to_numpy()).long().to(device)\n"
      ],
      "metadata": {
        "id": "eIYXpM5gmWmM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wCaQoRRBVGJm"
      },
      "outputs": [],
      "source": [
        "class EnvTM(nn.Module):\n",
        "    def __init__(self, num_topics, num_words, num_envs, device='cpu', empirical_bayes=True):\n",
        "        super(EnvTM, self).__init__()\n",
        "\n",
        "        def init_param(shape):\n",
        "            return nn.Parameter(torch.randn(shape, device=device))\n",
        "\n",
        "        def init_param_zeros(shape):\n",
        "            return nn.Parameter(torch.zeros(shape, device=device))\n",
        "\n",
        "        self.num_topics, self.num_words, self.num_envs = num_topics, num_words, num_envs\n",
        "\n",
        "        # Global Beta, β_{0,k} ~ 𝒩(·,·)\n",
        "        self.beta = init_param([num_topics, num_words])\n",
        "        self.beta_logvar = init_param_zeros([num_topics, num_words])\n",
        "        self.beta_prior = Normal(torch.zeros([num_topics, num_words], device=device), torch.ones([num_topics, num_words], device=device))\n",
        "\n",
        "        if empirical_bayes:\n",
        "            self.log_alpha_a = nn.Parameter(torch.tensor(1.0, device=device))\n",
        "            self.log_alpha_b = nn.Parameter(torch.tensor(1.0, device=device))\n",
        "        else:\n",
        "            alpha_a_fixed = torch.tensor(4.7, device=device)\n",
        "            alpha_b_fixed = torch.tensor(0.14, device=device)\n",
        "            self.log_alpha_a = alpha_a_fixed\n",
        "            self.log_alpha_b = alpha_b_fixed\n",
        "\n",
        "        # Sigma from Gamma distribution\n",
        "        self.sigma = torch.distributions.Gamma(torch.exp(self.log_alpha_a), torch.exp(self.log_alpha_b)).rsample([num_envs, num_topics, num_words])\n",
        "\n",
        "        # Initialize gamma with variance given by the inverse of sigma\n",
        "        self.gamma = init_param_zeros([num_envs, num_topics, num_words])\n",
        "        self.gamma_logvar = -torch.log(self.sigma).add(1e-8)\n",
        "        self.gamma_prior = Normal(torch.zeros_like(self.gamma), torch.sqrt(1.0/self.sigma).add(1e-8))\n",
        "\n",
        "        # Global Theta, θ_{d} ~ 𝒩(·,·)\n",
        "        self.theta_global_prior = Normal(torch.zeros(num_topics, device=device), torch.ones(num_topics, device=device))\n",
        "\n",
        "        self.theta_global_net = nn.Sequential(\n",
        "            nn.Linear(num_words, 50),\n",
        "            nn.BatchNorm1d(50),  # Batch Normalization Layer added after the Linear Layer and before ReLU\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, num_topics * 2)  # Produces both mean and variance\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, bow, env_index):\n",
        "        batch_size, vocab_size = bow.size()\n",
        "\n",
        "        # Compute document-specific theta_global mean and variance\n",
        "        self.theta_global_params = self.theta_global_net(bow)\n",
        "        theta_global_mu, theta_global_logvar = self.theta_global_params.split(self.num_topics, dim=-1)\n",
        "        theta_global_logvar = theta_global_logvar.add(1e-8)\n",
        "\n",
        "        # Sample 𝜃_{d} from the Normal distribution\n",
        "        theta_global = Normal(theta_global_mu, torch.exp(0.5 * theta_global_logvar).add(1e-8)).rsample()\n",
        "\n",
        "        theta_softmax = F.softmax(theta_global, dim=-1)\n",
        "\n",
        "        # Sample global beta (β₀,k)\n",
        "        beta_dist = Normal(self.beta, torch.exp(0.5 * self.beta_logvar).add(1e-8))\n",
        "        beta_sample = beta_dist.rsample()\n",
        "\n",
        "        # Generate gamma (γ_{e,k}) variance and sample gamma\n",
        "        gamma_dist = Normal(self.gamma[env_index], torch.exp(0.5 * self.gamma_logvar[env_index]).add(1e-8))\n",
        "        gamma_sample = gamma_dist.rsample()\n",
        "\n",
        "        beta_gamma_softmax = F.softmax(beta_sample + gamma_sample, dim=-1)\n",
        "\n",
        "        # Combine theta and beta_ek_sample\n",
        "        z = theta_softmax @ beta_gamma_softmax\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vZaYTDeR3qdD"
      },
      "outputs": [],
      "source": [
        "def calculate_kl_divergences(EnvTM, env, empirical_bayes=True):\n",
        "    theta_global_mu, theta_global_logvar = EnvTM.theta_global_params.split(EnvTM.num_topics, dim=-1)\n",
        "    theta_global_logvar = theta_global_logvar.add(1e-8)\n",
        "    theta_global = Normal(theta_global_mu, torch.exp(0.5 * theta_global_logvar).add(1e-8))\n",
        "    theta_global_kl = torch.distributions.kl.kl_divergence(theta_global, EnvTM.theta_global_prior).sum()\n",
        "\n",
        "    beta = Normal(EnvTM.beta, torch.exp(0.5 * EnvTM.beta_logvar))\n",
        "    beta_kl = torch.distributions.kl.kl_divergence(beta, EnvTM.beta_prior).sum()\n",
        "\n",
        "    if not empirical_bayes:\n",
        "        gamma = Normal(EnvTM.gamma, torch.exp(0.5 * EnvTM.gamma_logvar))\n",
        "        gamma_kl = torch.distributions.kl.kl_divergence(gamma, EnvTM.gamma_prior).sum()\n",
        "    else:\n",
        "        gamma_kl = 0  # No KL divergence for gamma when empirical_bayes is True\n",
        "\n",
        "    return theta_global_kl, beta_kl, gamma_kl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "32MtSHiD3saH"
      },
      "outputs": [],
      "source": [
        "def bbvi_update(minibatch, env_index, EnvTM, optimizer, n_samples):\n",
        "    optimizer.zero_grad()\n",
        "    elbo_accumulator = torch.zeros(1, device=minibatch.device)  # Initialize as tensor\n",
        "\n",
        "\n",
        "    # Get the unique environment indexes\n",
        "    unique_envs = torch.unique(env_index)\n",
        "\n",
        "    for env in unique_envs:\n",
        "        # Get the mask where current environment matches\n",
        "        mask = (env_index == env)\n",
        "\n",
        "        # Use the mask to get the current minibatch and number of samples\n",
        "        current_minibatch = minibatch[mask]\n",
        "        if current_minibatch.size(0) <= 1:\n",
        "            continue\n",
        "        current_n_samples = n_samples * mask.sum().item() / minibatch.size()[0]\n",
        "        # Pass the current minibatch to the model\n",
        "        z = EnvTM(current_minibatch, env)\n",
        "        theta_global_params = EnvTM.theta_global_net(current_minibatch)\n",
        "\n",
        "        kl_theta, kl_beta, kl_gamma = calculate_kl_divergences(env_tm_model, env, empirical_bayes=False)\n",
        "        elbo = (current_minibatch * z.log()).sum(-1).mul(current_n_samples).sub(kl_theta + kl_beta + kl_gamma)\n",
        "        elbo_accumulator += elbo.sum()\n",
        "\n",
        "    (-elbo_accumulator).backward(retain_graph=True)  # Add retain_graph=True here\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    return elbo_accumulator.item()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def empirical_bayes_update(EnvTM, optimizer_hyper, empirical_bayes=True, num_epochs_hyper=2, kl_threshold=1e-5):\n",
        "    \"\"\"Empirical Bayes update for the hyperparameters of the Gamma distribution.\"\"\"\n",
        "\n",
        "    if not empirical_bayes:\n",
        "        EnvTM.log_alpha_a = torch.log(torch.tensor(3.1, device=EnvTM.log_alpha_a.device) - 1)  # Convert alpha_a to its corresponding log_alpha_a\n",
        "        EnvTM.log_alpha_b = torch.log(torch.tensor(0.29, device=EnvTM.log_alpha_b.device) - 1)  # Convert alpha_b to its corresponding log_alpha_b\n",
        "        return\n",
        "\n",
        "    previous_gamma_kl = float('inf')\n",
        "\n",
        "    for _ in range(num_epochs_hyper):\n",
        "        optimizer_hyper.zero_grad()\n",
        "\n",
        "        # Sample sigma from the updated gamma distribution\n",
        "        sigma_sample = torch.distributions.Gamma(torch.nn.functional.softplus(EnvTM.log_alpha_a), torch.nn.functional.softplus(EnvTM.log_alpha_b)).rsample([EnvTM.num_envs, EnvTM.num_topics, EnvTM.num_words])\n",
        "\n",
        "        gamma_prior = Normal(torch.zeros_like(EnvTM.gamma), torch.sqrt(1.0/sigma_sample).add(1e-8))\n",
        "\n",
        "        # KL divergence for gamma\n",
        "        gamma = Normal(EnvTM.gamma, torch.exp(0.5 * EnvTM.gamma_logvar))\n",
        "        gamma_kl = torch.distributions.kl.kl_divergence(gamma, gamma_prior).sum()\n",
        "        # print('gamma_kl', gamma_kl.item())\n",
        "\n",
        "        delta_gamma_kl = torch.abs(gamma_kl - previous_gamma_kl).item()\n",
        "\n",
        "        if delta_gamma_kl < kl_threshold:\n",
        "            print(\"Early stopping of hyperparameter updates based on gamma KL divergence stability.\")\n",
        "            break\n",
        "\n",
        "        (-gamma_kl).backward(retain_graph=True)\n",
        "        optimizer_hyper.step()\n",
        "\n",
        "        previous_gamma_kl = gamma_kl.item()\n"
      ],
      "metadata": {
        "id": "YIVo7YjjNDyr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NUaAWQtzKN_u"
      },
      "outputs": [],
      "source": [
        "def train_model(EnvTM, docs_word_matrix_tensor, env_index_tensor, num_epochs=80, minibatch_size=1024, lr=0.01, empirical_bayes=True):\n",
        "    EnvTM = EnvTM.to(device)  # move your model to the GPU\n",
        "    optimizer = torch.optim.Adam(EnvTM.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "\n",
        "    # Separate optimizer for hyperparameters\n",
        "    optimizer_hyper = torch.optim.Adam([EnvTM.log_alpha_a, EnvTM.log_alpha_b], lr=lr, betas=(0.9, 0.999))\n",
        "\n",
        "    docs_word_matrix_tensor = docs_word_matrix_tensor.to(device)  # move your tensors to the GPU\n",
        "    env_index_tensor = env_index_tensor.to(device)  # move your tensors to the GPU\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        elbo_accumulator = 0.0\n",
        "\n",
        "        # Randomly permute data for minibatches\n",
        "        permutation = torch.randperm(docs_word_matrix_tensor.size()[0])\n",
        "\n",
        "        for i in range(0, docs_word_matrix_tensor.size()[0], minibatch_size):\n",
        "            # Get minibatch\n",
        "            indices = permutation[i:i+minibatch_size]\n",
        "            minibatch = docs_word_matrix_tensor[indices]\n",
        "\n",
        "            # Get corresponding environment indices\n",
        "            minibatch_env_index = env_index_tensor[indices]\n",
        "\n",
        "            # BBVI update for the minibatch\n",
        "            elbo = bbvi_update(minibatch, minibatch_env_index, EnvTM, optimizer, docs_word_matrix_tensor.size()[0])\n",
        "\n",
        "            # Accumulate ELBO\n",
        "            elbo_accumulator += elbo\n",
        "\n",
        "        # Empirical Bayes Step\n",
        "        if empirical_bayes:\n",
        "            empirical_bayes_update(EnvTM, optimizer_hyper)\n",
        "\n",
        "        # Calculate average ELBO for the epoch\n",
        "        avg_elbo = elbo_accumulator / (docs_word_matrix_tensor.size()[0] / minibatch_size)\n",
        "\n",
        "        print(f'Epoch: {epoch+1}, Average ELBO: {avg_elbo}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "empirical_bayes = False\n",
        "\n",
        "num_topics = 20\n",
        "num_envs = 2\n",
        "\n",
        "if empirical_bayes:\n",
        "    num_epochs = 15\n",
        "else:\n",
        "    num_epochs = 150"
      ],
      "metadata": {
        "id": "SbiRUzxOqShF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_tm_model = EnvTM(num_topics=num_topics, num_words=len(vectorizer.get_feature_names_out()), num_envs=num_envs, device=device, empirical_bayes=empirical_bayes)\n",
        "\n",
        "train_model(env_tm_model, docs_word_matrix_tensor, env_index_tensor, num_epochs=num_epochs, minibatch_size=1024, lr=0.01)"
      ],
      "metadata": {
        "id": "6H2e8NcS3FUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softplus(x):\n",
        "    return math.log(1 + math.exp(x))\n",
        "\n",
        "if not empirical_bayes:\n",
        "    alpha_a = env_tm_model.log_alpha_a.item()\n",
        "    alpha_b = env_tm_model.log_alpha_b.item()\n",
        "    print(alpha_a)\n",
        "    print(alpha_b)\n",
        "else:\n",
        "    alpha_a_softplus = softplus(env_tm_model.log_alpha_a.item())\n",
        "    alpha_b_softplus = softplus(env_tm_model.log_alpha_b.item())\n",
        "    print(f\"After Training (softplus): log_alpha_a = {alpha_a_softplus}, log_alpha_b = {alpha_b_softplus}\")\n"
      ],
      "metadata": {
        "id": "C9nJfzMfee_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_word_matrix_raw = vectorizer.transform(test1['text'])\n",
        "test_data_word_matrix_tensor = torch.from_numpy(test_data_word_matrix_raw.toarray()).float().to(device)\n"
      ],
      "metadata": {
        "id": "W6wpUBux3HoP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(env_tm_model, test_data_word_matrix_tensor):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    env_tm_model.to(device)\n",
        "    env_tm_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        theta_test_params = env_tm_model.theta_global_net(test_data_word_matrix_tensor)\n",
        "        theta_test_mu, theta_test_logvar = theta_test_params.split(env_tm_model.num_topics, dim=-1)\n",
        "        theta_test_dist = Normal(theta_test_mu, torch.exp(0.5 * theta_test_logvar).add(1e-8))\n",
        "        theta_test = theta_test_dist.rsample()\n",
        "        theta_test_softmax = F.softmax(theta_test, dim=-1)\n",
        "        beta_test_softmax = F.softmax(env_tm_model.beta.to(device), dim=-1)\n",
        "\n",
        "        likelihood = torch.mm(theta_test_softmax, beta_test_softmax)\n",
        "        N = torch.sum(test_data_word_matrix_tensor)\n",
        "        log_perplex = -torch.sum(torch.log(likelihood) * test_data_word_matrix_tensor) / N\n",
        "        perplexity = torch.exp(log_perplex)\n",
        "\n",
        "    return perplexity, theta_test_softmax\n",
        "\n",
        "def evaluate_model_with_gamma_per_env(env_tm_model, test_data_word_matrix_tensor):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    env_tm_model.to(device)\n",
        "    env_tm_model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        theta_test_params = env_tm_model.theta_global_net(test_data_word_matrix_tensor)\n",
        "        theta_test_mu, theta_test_logvar = theta_test_params.split(env_tm_model.num_topics, dim=-1)\n",
        "        theta_test_dist = Normal(theta_test_mu, torch.exp(0.5 * theta_test_logvar).add(1e-8))\n",
        "        theta_test = theta_test_dist.rsample()\n",
        "        theta_test_softmax = F.softmax(theta_test, dim=-1)\n",
        "\n",
        "        gamma_learned = env_tm_model.gamma[0]\n",
        "\n",
        "        beta_gamma_test_softmax = F.softmax(env_tm_model.beta.to(device) + gamma_learned, dim=-1)\n",
        "        log_likelihood = torch.mm(theta_test_softmax, beta_gamma_test_softmax)\n",
        "        N = torch.sum(test_data_word_matrix_tensor)\n",
        "        log_perplex = -torch.sum(torch.log(log_likelihood) * test_data_word_matrix_tensor) / N\n",
        "        perplexity = torch.exp(log_perplex)\n",
        "\n",
        "    return perplexity\n"
      ],
      "metadata": {
        "id": "zDFxIWUV3Hqq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity, theta_test_softmax = evaluate_model(env_tm_model, test_data_word_matrix_tensor)\n",
        "perplexities_by_env = evaluate_model_with_gamma_per_env(env_tm_model, test_data_word_matrix_tensor)\n",
        "\n",
        "print(f'Perplexity for environment {0} effects: {perplexities_by_env}')\n",
        "\n",
        "print(f'Test Perplexity: {perplexity}')"
      ],
      "metadata": {
        "id": "oPg2aVJymm1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_top_words(env_tm_model, vectorizer, num_top_words):\n",
        "    global_beta = torch.nn.functional.softmax(env_tm_model.beta, dim=1)  # Convert to probabilities\n",
        "    gamma = env_tm_model.gamma\n",
        "\n",
        "    # Print top words for global beta\n",
        "    print(\"Top words for global beta:\")\n",
        "    for i, topic in enumerate(global_beta):\n",
        "        top_words = topic.topk(num_top_words).indices\n",
        "        print(f'Topic {i+1}: {[vectorizer.get_feature_names_out()[i] for i in top_words]}')\n",
        "\n",
        "    # Print top words for gamma\n",
        "    print(\"\\nTop words for gamma:\")\n",
        "    for env_index, env_gamma in enumerate(gamma):\n",
        "        print(f\"Environment {env_index+1}:\")\n",
        "        for i, topic in enumerate(env_gamma):\n",
        "            top_words = topic.topk(num_top_words).indices\n",
        "            print(f'Topic {i+1}: {[vectorizer.get_feature_names_out()[i] for i in top_words]}')\n",
        "        print()"
      ],
      "metadata": {
        "id": "6z_Dtm4v3iSf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_top_words(env_tm_model, vectorizer, num_top_words=11)"
      ],
      "metadata": {
        "id": "JMNSvCWQ3nmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(data):\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "def get_top_indices_values(arr, top_n=8):\n",
        "    indices = np.argsort(-arr)[:top_n]\n",
        "    values = arr[indices]\n",
        "    return indices, values\n",
        "\n",
        "def get_words(vocabulary, indices):\n",
        "    return [vocabulary[i] for i in indices]\n",
        "\n",
        "def plot_gamma_beta_heatmaps(gamma_data, beta_data, words, title):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # Defining the color scale between 0 and 1\n",
        "    im1 = ax1.imshow(gamma_data.T, cmap='hot', interpolation='nearest', vmin=0, vmax=1)\n",
        "    im2 = ax2.imshow(beta_data.T.reshape(-1, 1), cmap='hot', interpolation='nearest', vmin=0, vmax=1)\n",
        "\n",
        "    num_environments = gamma_data.shape[0]\n",
        "    environments = [f'Environment {i}' for i in range(num_environments)]\n",
        "\n",
        "    # Settings for gamma heatmap\n",
        "    ax1.set_yticks(np.arange(len(words)))\n",
        "    ax1.set_xticks(np.arange(num_environments))\n",
        "    ax1.set_yticklabels(words)\n",
        "    ax1.set_xticklabels(environments)\n",
        "    plt.setp(ax1.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "    cbar1 = fig.colorbar(im1, ax=ax1)\n",
        "    cbar1.ax.set_ylabel(\"Normalized Gamma\", rotation=-90, va=\"bottom\")\n",
        "\n",
        "    # Settings for beta grid\n",
        "    ax2.set_yticks(np.arange(len(words)))\n",
        "    ax2.set_xticks([0])\n",
        "    ax2.set_yticklabels(words)\n",
        "    ax2.set_xticklabels(['Beta'])\n",
        "    plt.setp(ax2.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "    cbar2 = fig.colorbar(im2, ax=ax2)\n",
        "    cbar2.ax.set_ylabel(\"Normalized Beta\", rotation=-90, va=\"bottom\")\n",
        "\n",
        "    ax1.set_title(title)\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_topic(lda, vocabulary, topic_index, top_n=8):\n",
        "    # Normalize the entire beta array for the specific topic\n",
        "    beta_values = normalize(lda.beta[topic_index, :].cpu().detach().numpy())\n",
        "\n",
        "    # Normalize the entire gamma arrays for the specific topic in all environments\n",
        "    num_environments = lda.gamma.shape[0]\n",
        "    gamma_values = [normalize(lda.gamma[i, topic_index, :].cpu().detach().numpy()) for i in range(num_environments)]\n",
        "\n",
        "    # Get the top beta indices and values\n",
        "    beta_indices, _ = get_top_indices_values(beta_values, top_n)\n",
        "\n",
        "    for env_index, gamma_value in enumerate(gamma_values):\n",
        "        # Get the top gamma indices and values\n",
        "        gamma_indices, _ = get_top_indices_values(gamma_value, top_n)\n",
        "\n",
        "        # Get the corresponding words from the vocabulary\n",
        "        gamma_words = get_words(vocabulary, gamma_indices)\n",
        "        beta_words = get_words(vocabulary, beta_indices)\n",
        "\n",
        "        # Print the top words\n",
        "        print(f\"Top words in gamma environment {env_index}:\", gamma_words)\n",
        "        print(\"Top words in beta:               \", beta_words)\n",
        "\n",
        "        # Get the gamma and beta values for top words\n",
        "        gamma_values_top_words = [gamma_values[i][gamma_indices] for i in range(num_environments)]\n",
        "        beta_values_top_words = beta_values[gamma_indices]\n",
        "\n",
        "        # Plot the heatmaps\n",
        "        plot_gamma_beta_heatmaps(np.array(gamma_values_top_words), beta_values_top_words, gamma_words, f\"Environment {env_index}: Top Words\")\n",
        "\n",
        "    # Gamma and Beta values for top words in beta\n",
        "    gamma_values_beta = [gamma_values[i][beta_indices] for i in range(num_environments)]\n",
        "    beta_values_beta = beta_values[beta_indices]\n",
        "    beta_words = get_words(vocabulary, beta_indices)\n",
        "\n",
        "    # Plot the heatmaps for the top words in beta\n",
        "    plot_gamma_beta_heatmaps(np.array(gamma_values_beta), beta_values_beta, beta_words, \"Beta: Top Words\")\n",
        "\n",
        "\n",
        "vocabulary = list(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Analyzing topic 4 with 8 top words\n",
        "analyze_topic(env_tm_model, vocabulary, topic_index=10, top_n=8)"
      ],
      "metadata": {
        "id": "q96oqQll3qVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_gamma_per_environment(model, threshold=1e-2):\n",
        "    gamma_values = model.gamma.detach().cpu().numpy()\n",
        "\n",
        "    for env_index in range(gamma_values.shape[0]):\n",
        "        print(f\"Environment {env_index}:\")\n",
        "        gamma_env_values = gamma_values[env_index]\n",
        "\n",
        "        close_to_zero = np.abs(gamma_env_values) < threshold\n",
        "        sparsity_percentage = 100 * np.sum(close_to_zero) / gamma_env_values.size\n",
        "\n",
        "        print(f\"Sparsity Percentage: {sparsity_percentage}%\")\n",
        "        print(f\"Mean of Gamma: {np.mean(gamma_env_values)}\")\n",
        "        print(f\"Standard Deviation of Gamma: {np.std(gamma_env_values)}\")\n",
        "        plt.hist(gamma_env_values.flatten(), bins=50)\n",
        "        plt.title(f\"Histogram of Gamma Values for Environment {env_index}\")\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "58IWyzWk3wDS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_gamma_per_environment(env_tm_model)\n"
      ],
      "metadata": {
        "id": "aLcTjGDt3zSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_indices_values(arr, top_n=10):\n",
        "    \"\"\"\n",
        "    Get the indices and values of the top elements in an array.\n",
        "    \"\"\"\n",
        "    indices = np.argsort(-arr)[:top_n]\n",
        "    values = arr[indices]\n",
        "    return indices, values\n",
        "\n",
        "def get_words(vocabulary, indices):\n",
        "    \"\"\"\n",
        "    Get words from the vocabulary corresponding to given indices.\n",
        "    \"\"\"\n",
        "    return [vocabulary[i] for i in indices]\n",
        "\n",
        "def evaluate_documents_with_topic(env_tm_model, test_set, topic_id):\n",
        "    \"\"\"\n",
        "    Evaluate documents in a test set for a specific topic, and create DataFrames for top words and their counts.\n",
        "    \"\"\"\n",
        "   # Preprocess test set\n",
        "    test_data_word_matrix_raw = vectorizer.transform(test_set['text'])\n",
        "    test_data_word_matrix_tensor = torch.from_numpy(test_data_word_matrix_raw.toarray()).float().to(device)\n",
        "\n",
        "    # Evaluate model\n",
        "    perplexity, theta_test_softmax = evaluate_model(env_tm_model, test_data_word_matrix_tensor)\n",
        "\n",
        "    # Identify documents where the topic has the highest theta value\n",
        "    highest_theta_docs_indices = [doc_id for doc_id, theta in enumerate(theta_test_softmax) if torch.argmax(theta) == topic_id]\n",
        "\n",
        "    # Create a DataFrame with selected documents\n",
        "    # Ensuring that indices are within the range and exist in the test set\n",
        "    document_df = pd.DataFrame({\n",
        "        'Document ID': highest_theta_docs_indices,\n",
        "        'Document Text': [test_set['text'].iloc[doc_id] for doc_id in highest_theta_docs_indices if doc_id < len(test_set)]\n",
        "    })\n",
        "    # Get top words for each gamma environment and count their occurrences\n",
        "    word_counts_dfs = []\n",
        "    for gamma_id, gamma in enumerate(env_tm_model.gamma):\n",
        "        top_words = get_top_words_for_gamma(gamma, topic_id)\n",
        "        word_counts = count_words_in_documents(document_df['Document Text'], top_words)\n",
        "        word_count_df = pd.DataFrame({'Word': top_words, f'Gamma {gamma_id} Count': word_counts})\n",
        "        word_counts_dfs.append(word_count_df)\n",
        "\n",
        "    return document_df, word_counts_dfs\n",
        "\n",
        "def get_top_words_for_gamma(gamma, topic_id, num_words=10):\n",
        "    \"\"\"\n",
        "    Get top words for a specific topic based on gamma probabilities.\n",
        "    \"\"\"\n",
        "    gamma_values = gamma[topic_id].detach().cpu().numpy()\n",
        "    top_indices, _ = get_top_indices_values(gamma_values, num_words)\n",
        "    vocabulary = vectorizer.get_feature_names_out()\n",
        "    return get_words(vocabulary, top_indices)\n",
        "\n",
        "def count_words_in_documents(documents, words):\n",
        "    \"\"\"\n",
        "    Count the occurrences of specific words in a list of documents.\n",
        "    \"\"\"\n",
        "    word_counts = {word: 0 for word in words}\n",
        "    for doc in documents:\n",
        "        for word in words:\n",
        "            word_counts[word] += doc.count(word)\n",
        "    return [word_counts[word] for word in words]"
      ],
      "metadata": {
        "id": "q1EMaZgG302Y"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_and_compare_documents(env_tm_model, test1, test2, num_topics):\n",
        "    \"\"\"\n",
        "    Evaluate and compare documents across two test sets for all topics, focusing on the top 10 words of each topic.\n",
        "    \"\"\"\n",
        "    comparison_results = []\n",
        "\n",
        "    for topic_id in range(num_topics):\n",
        "        # Evaluate documents for this topic in both test sets\n",
        "        _, word_counts_dfs_test1 = evaluate_documents_with_topic(env_tm_model, test1, topic_id)\n",
        "        _, word_counts_dfs_test2 = evaluate_documents_with_topic(env_tm_model, test2, topic_id)\n",
        "\n",
        "        for gamma_id in range(len(env_tm_model.gamma)):\n",
        "            # Limit to top 10 words for the current gamma environment\n",
        "            top_words_df_test1 = word_counts_dfs_test1[gamma_id].nlargest(10, f'Gamma {gamma_id} Count')\n",
        "            top_words_df_test2 = word_counts_dfs_test2[gamma_id].nlargest(10, f'Gamma {gamma_id} Count')\n",
        "\n",
        "            # Combine the DataFrames for the current gamma environment\n",
        "            combined_df = pd.merge(top_words_df_test1, top_words_df_test2, on='Word', how='outer')\n",
        "            combined_df.columns = ['Word', f'Gamma {gamma_id} Count (Right Leaning)', f'Gamma {gamma_id} Count (Left Leaning)']\n",
        "            combined_df.fillna(0, inplace=True)\n",
        "            combined_df = combined_df.astype({f'Gamma {gamma_id} Count (Right Leaning)': 'int', f'Gamma {gamma_id} Count (Left Leaning)': 'int'})\n",
        "\n",
        "            # Count occurrences where a word has a higher count in the opposite environment\n",
        "            if gamma_id == 0:\n",
        "                # For words in the Republican environment, check if they have higher counts in the Democrat environment\n",
        "                count_opposite = combined_df[combined_df[f'Gamma {gamma_id} Count (Right Leaning)'] < combined_df[f'Gamma {gamma_id} Count (Left Leaning)']].shape[0]\n",
        "            else:\n",
        "                # For words in the Democrat environment, check if they have higher counts in the Republican environment\n",
        "                count_opposite = combined_df[combined_df[f'Gamma {gamma_id} Count (Right Leaning)'] > combined_df[f'Gamma {gamma_id} Count (Left Leaning)']].shape[0]\n",
        "\n",
        "            total_words = combined_df.shape[0]\n",
        "            fraction_opposite = count_opposite / total_words if total_words > 0 else 0\n",
        "            comparison_results.append((topic_id, gamma_id, fraction_opposite, count_opposite, total_words))\n",
        "\n",
        "    return pd.DataFrame(comparison_results, columns=['Topic ID', 'Gamma ID', 'Fraction Opposite', 'Count Opposite', 'Total Words'])\n"
      ],
      "metadata": {
        "id": "j4ery0I2cl9r"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = 20  # Replace with the actual number of topics\n",
        "comparison_df = evaluate_and_compare_documents(env_tm_model, test1, test2, num_topics)"
      ],
      "metadata": {
        "id": "SYf4JlNUht5l"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "median_fraction_opposite = comparison_df['Fraction Opposite'].median()\n",
        "median_count_opposite = comparison_df['Count Opposite'].median()\n",
        "print(f\"Median Fraction Opposite: {median_fraction_opposite}\")\n",
        "print(f\"Median Count Opposite: {median_count_opposite}\")\n"
      ],
      "metadata": {
        "id": "46pkefd5iYYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PQWueNKeif63"
      },
      "execution_count": 31,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}